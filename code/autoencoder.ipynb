{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Sat, May 2, 16:54:30 2019\n",
    "@author: Surya Pratap Singh\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math # for performing mathematical operations like round(), sin() etc\n",
    "import matplotlib.pyplot as plt # for plotting and visualizing\n",
    "import tensorflow as tf # using tensorflow\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing mnist dataset\n",
    "\n",
    "training_data = pd.read_csv(\"../input/train.csv\")\n",
    "\n",
    "train_data = (training_data.iloc[0:math.floor(len(training_data.index)*0.7),1:].values, \n",
    "              training_data.iloc[0:math.floor(len(training_data.index)*0.7),0].values)\n",
    "test_data = (training_data.iloc[math.floor(len(training_data.index)*0.7)+1:,1:].values, \n",
    "             training_data.iloc[math.floor(len(training_data.index)*0.7)+1:,0].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29399, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = tf.placeholder(tf.int64)\n",
    "num_features =  len(training_data.columns)-1\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices( X ).batch(batch_size).repeat()\n",
    "\n",
    "iter1 = training_dataset.make_initializable_iterator()\n",
    "features = iter1.get_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "defining the number of units in each layer, learning rate and \n",
    "the activation function to be used. All of this defines the basic \n",
    "structure of the graph\n",
    "'''\n",
    "\n",
    "input_features = 784 #28x28 pixels\n",
    "hidden_units1 = 392\n",
    "hidden_units2 = 196\n",
    "hidden_units3 = 98\n",
    "hidden_units4 = hidden_units2\n",
    "hidden_units5 = hidden_units1\n",
    "output_units = input_features\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "actf = tf.nn.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.variance_scaling_initializer() #this is done to make the \n",
    "#variance of the outputs equla to that of variance of inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the weights of each layer\n",
    "wt1 = tf.Variable(initializer([input_features, hidden_units1]), dtype = tf.float32)\n",
    "wt2 = tf.Variable(initializer([hidden_units1, hidden_units2]), dtype = tf.float32)\n",
    "wt3 = tf.Variable(initializer([hidden_units2, hidden_units3]), dtype = tf.float32)\n",
    "wt4 = tf.Variable(initializer([hidden_units3, hidden_units4]), dtype = tf.float32)\n",
    "wt5 = tf.Variable(initializer([hidden_units4, hidden_units5]), dtype = tf.float32)\n",
    "wt6 = tf.Variable(initializer([hidden_units5, output_units]), dtype = tf.float32)\n",
    "\n",
    "#defining the biases for each layer\n",
    "b1 = tf.Variable(tf.zeros(hidden_units1))\n",
    "b2 = tf.Variable(tf.zeros(hidden_units2))\n",
    "b3 = tf.Variable(tf.zeros(hidden_units3))\n",
    "b4 = tf.Variable(tf.zeros(hidden_units4))\n",
    "b5 = tf.Variable(tf.zeros(hidden_units5))\n",
    "b6 = tf.Variable(tf.zeros(output_units))\n",
    "\n",
    "#defining feedforward\n",
    "hidden_units1 = actf(tf.matmul(features,wt1)+b1)\n",
    "hidden_units2 = actf(tf.matmul(hidden_units1,wt2)+b2)\n",
    "hidden_units3 = actf(tf.matmul(hidden_units2,wt3)+b3)\n",
    "hidden_units4 = actf(tf.matmul(hidden_units3,wt4)+b4)\n",
    "hidden_units5 = actf(tf.matmul(hidden_units4,wt5)+b5)\n",
    "output_units = actf(tf.matmul(hidden_units5,wt6)+b6)\n",
    "\n",
    "#defining loss function\n",
    "loss = tf.reduce_mean(tf.square(output_units - features))\n",
    "\n",
    "#defining the optimizing function\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "epoch_num = 5\n",
    "BATCH_SIZE = 150\n",
    "num_test_images = 10\n",
    "num_batches = math.floor(training_data.shape[0]*0.7)//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "0\n",
      "Iter: 0, Loss: 2340.3729\n",
      "1\n",
      "Iter: 1, Loss: 1209.8279\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(iter1.initializer, feed_dict={ X: train_data[0], batch_size: BATCH_SIZE})\n",
    "    print('Training...')\n",
    "    for epoch in range(epoch_num):\n",
    "        print(epoch)\n",
    "        tot_loss = 0\n",
    "        for _ in range(num_batches):\n",
    "            _, loss_value = sess.run([train, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(epoch, tot_loss / num_batches))\n",
    "    \n",
    "    # initialise iterator with test data\n",
    "    sess.run(iter1.initializer, feed_dict={ X: test_data[0], batch_size: num_test_images})\n",
    "    results = sess.run(output_units)\n",
    "    \n",
    "    #let us see how the constructions look like\n",
    "    f, a = plt.subplots(2,10,figsize=(20,4))\n",
    "    for i in range(10):\n",
    "        a[0][i].imshow(np.reshape(test_data[0][i],(28,28)))\n",
    "        a[1][i].imshow(np.reshape(results[i],(28,28)))\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
